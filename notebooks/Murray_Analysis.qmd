---
title: "Visual Analysis"
author: "Rodney Murray"
format: docx
---

```{r setup, include=FALSE}
#Retain this code chunk!!!
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{r}
library(tidyverse)
library(lme4)
library(broom)
library(MuMIn)
library(broom.mixed)
library(GGally)
library(corrplot)

```

# Data Loading and Processing


## Load Data
```{r}
# ------------------------------
# Data Loading and Preprocessing
# ------------------------------

# Load data
df <- read.csv('../src/data/WittData_AHRev052324.csv')
dots <- read.csv('../src/data/Witt_Dots.csv')

```

## Processing
Calculate which quadrant each dot is in
```{r}
# Calculate the quadrant that each dot is in
dots <- dots %>% mutate(
  quadrant = case_when(
  x < 50 & y >= 50 ~ 'A',
  x >= 50 & y >= 50 ~ 'B',
  x < 50 & y < 50 ~ 'C',
  x >= 50 & y < 50 ~ 'D',
  )
)

```

Calculate the mean values for x and y and which quadrant the centroid is in
```{r}
# Calculate the centroid for each image and the corresponding quadrant
mean_dots <- dots %>% 
  group_by(currImg) %>% 
  summarise(meanx = mean(x), meany = mean(y)) %>% 
  mutate(
      mean_quadrant = case_when(
        meanx < 50 & meany >= 50 ~ 'A',
        meanx >= 50 & meany >= 50 ~ 'B',
        meanx < 50 & meany < 50 ~ 'C',
        meanx >= 50 & meany < 50 ~ 'D',
    )
  )

```

Join the centroid and associated quadrant back to the dots
```{r}
# Jone the centroid and quadrant back to the dots
full_dots <- left_join(dots, mean_dots, by = 'currImg') %>% 
  mutate(
    in_mean_quadrant = quadrant == mean_quadrant
    )
head(full_dots)

# Calculate the number of dots in the same quadrant as the centroid
mean_full_dots <- full_dots %>% 
  group_by(currImg) %>% 
  summarise(n_in_quadrant = sum(in_mean_quadrant), n=n())
head(mean_full_dots)

```


Calculate if the special groups are in the same quadrant as the mean of all
```{r}
# Calculate if the special groups are in the same quadrant as the mean of all to double check the input data
group_dots <- dots %>% 
  group_by(currImg, cond) %>% 
  summarize(meanx=mean(x), meany=mean(y)) %>% 
  mutate(
    group_quadrant = case_when(
      meanx < 50 & meany >= 50 ~ 'A',
      meanx >= 50 & meany >= 50 ~ 'B',
      meanx < 50 & meany < 50 ~ 'C',
      meanx >= 50 & meany < 50 ~ 'D',
    )
  )%>% 
  pivot_wider(
    id_cols=currImg,
    names_from=cond,
    values_from=group_quadrant
  )
group_dots %>% head()

```



Join the dot data back with the response data
```{r}
# Join the dot data back with the response data
full_df <- left_join(df, mean_full_dots, by = 'currImg') %>% 
  left_join(group_dots, by='currImg') %>%
  mutate(
    correct = as.numeric(resp == corrQuadREV),
    distToNearest = pmin(abs(meanx -50), abs(meany - 50)),
    isColSameREV = as.numeric(color == corrQuadREV),
    isSizeSameREV = as.numeric(size == corrQuadREV),
    isPulseSameREV = as.numeric(pulse == corrQuadREV)
    )
full_df %>% head() %>% select(isColSame, isColSameREV, correct)

```


Interestingly enough it seems that the special groups were correctly calculated
```{r}
# Doulbe check that the special groups were correctly calculated
full_df %>% 
  mutate(
    color = isColSame == isColSameREV,
    size = isSizeSame == isSizeSameREV,
    pulse = isPulseSame == isPulseSameREV
    ) %>% 
  select(color, size, pulse, isColSameREV, isSizeSameREV, isPulseSameREV) %>% 
  summary()

```




Plot our data to check that we correctly identified the quadrants for the centroids
```{r}
# Plot the centroids for all images and color by the quadrant for QC
ggplot(df, aes(meanx, meany, color=corrQuadREV)) +
  geom_point()

```

## New Variable Explanation

```{r}
# Take a look at the new variables for a single image
single_img <- full_dots %>% filter(currImg == 4)

ggplot(single_img, aes(x, y, color=in_mean_quadrant)) +
  geom_point() +
  labs(title='Example of N-in-quadrant', color='') +
  geom_point(aes(x=meanx, y=meany, color='Centroid'), size=3) +
  scale_color_manual(values=c('dodgerblue', 'grey', 'red')) +
  geom_vline(xintercept=50, color='black') +
  geom_hline(yintercept=50, color='black') +
  lims(x=c(0, 100), y=c(0, 100)) +
  theme_minimal(base_size=18)
ggsave('../presentation/figures/n_in_quadrant.png')

```



# EDA

```{r}
# ------------------------------
# EDA
# ------------------------------

# Iterate over each column in cols and calculate the proportion correct for binned values of each column
# This will help us identify potential relationships between the continuous variables and the probability the subject correctly identified the quadrant
cols <- c('distToNearest', 'distToMiddleREV', 'n_in_quadrant')
summaries <- data.frame()
for (i in 1:length(cols)) {
  col <- cols[i]
  df_summary <- full_df %>% 
    mutate(col_bin = cut(!!sym(col), breaks = 15)) %>% 
    group_by(col_bin) %>%
    summarise(
      prop_correct = mean(correct),
      mean_value = mean(!!sym(col)),
      col_name = col
    )
  summaries <- bind_rows(summaries, df_summary)
}

ggplot(summaries, aes(mean_value, prop_correct)) +
  facet_wrap(~col_name, scales='free_x', ncol=2) +
  geom_point() +
  theme_minimal()

```

```{r}
# Take a look at the correlation between the fixed effects
corrs <- full_df %>% 
  select(isPulseSame, isColSame, howManyCorr, isSizeSame, distToNearest, n_in_quadrant, distToMiddleREV) %>% 
  cor()
corrplot(corrs, type='upper')

```


# Modeling

I left out `howManyCorr` becuase of collinearity, and the currImg due to convergence issues.

Not too worried about currImg being out since we have other variables which should help describe the data reasonbly well.

```{r}
# ------------------------------
# Modeling
# ------------------------------

# Fit a full model with all potential fixed effects that we want to consider
formula <- correct ~ 
  distToMiddleREV + 
  n_in_quadrant +
  meanSD +
  distToNearest +
  isColSame +
  isSizeSame +
  isPulseSame +
  (1|subj)

model <- glmer(
  formula=formula,
  data = full_df,
  family = binomial,
  )
summary(model)

```

```{r}
# calculate a psuedo R^2 for the glmer model
r.squaredGLMM(model)

```

## Checkout a transformation

Try sqrt or log transformation of the n_in_quadrant to see if it makes a difference
```{r}
trans_formula <- correct ~ 
  distToMiddleREV + 
  log(n_in_quadrant) +
  log(distToNearest) +
  isColSame +
  isSizeSame +
  isPulseSame +
  (1|subj)

trans_model <- glmer(
  formula=trans_formula,
  data = full_df,
  family = binomial,
  )
summary(trans_model)
```


## Model Selection

### Dredge


```{r}
# Use Dredge to find the best model
options(na.action='na.fail')
dredge_res <- dredge(model, fixed=c('isColSame', 'isPulseSame', 'isSizeSame'))
dredge_res

```

Plot a heat map of the coefficients for each model
```{r}
# plot a heatmap of the coefficients for each model in the dredge results
dredge_df <- as.data.frame(dredge_res) %>% 
  select(-df, -logLik, -delta, -weight) %>% 
  mutate(
    model_id=row_number(),
    has_n_in_quadrant = !is.na(n_in_quadrant)
    ) %>% 
  pivot_longer(cols=c('(Intercept)', 'distToMiddleREV', 'distToNearest', 'isColSame', 'isPulseSame', 'isSizeSame', 'meanSD', 'n_in_quadrant'), names_to='term', values_to='estimate') %>% 
  filter(term != '(Intercept)') %>% 
  mutate(
    odds_ratio = exp(estimate),
  ) %>% 
  arrange(AICc)
dredge_df %>% head()
```

```{r}
ggplot(dredge_df, aes(x=term, y=model_id, fill=estimate)) +
  geom_tile() +
  scale_fill_gradient2(low='red', mid='white', high='dodgerblue', midpoint=0) +
  theme_minimal() +
  labs(title='Dredge Results', x='Rank', y='Term')
```

```{r}
ggplot(dredge_df, aes(x=odds_ratio, y=term, color=has_n_in_quadrant)) +
  # geom_point() +
  geom_jitter(
    width=0, height=0.25
  ) +
  scale_x_log10() +
  geom_vline(xintercept=1, color='black') + 
  labs(x='Odds Ratio', y='Term', title='Illustration of Effect of N in Quadrant on other variables')
ggsave('../presentation/figures/dredge_estimates.png')
```


```{r}
best_model <- get.models(dredge_res, 1)[[1]]
summary(best_model)

```


```{r}
# Calculate confidence intervals for the model using bootstrapping methods


# This takes a while to run, only run once, then save results and load later when re-running
# conf_ints <- confint(
#   best_model, 
#   oldNames=F, 
#   method='boot', 
#   parallel='multicore',
#   ncpus=8,
#   .progress='txt'
#   )
# save(conf_ints, file='conf_ints.RData')
# conf_ints
load('conf_ints.RData')

```


```{r}
# Tidy up the confidence intervals and join with the coefficients for plots
conf_df <- as.data.frame(conf_ints)
colnames(conf_df) <- c('conf.low', 'conf.high')
conf_df <- conf_df %>% rownames_to_column('term')
boot_confint <- tidy(best_model) %>% left_join(conf_df, by='term')

```

```{r}
# Classify the coeficients and provide more meaningful names for the plots
boot_conf_plot <- boot_confint %>% 
  filter(!(term %in% c('(Intercept)', 'sd__(Intercept)'))) %>% 
  mutate(
    term_class = case_when(
      term == 'isColSame' ~ 'categorical',
      term == 'isSizeSame' ~ 'categorical',
      term == 'isPulseSame' ~ 'categorical',
      term == 'distToMiddleREV' ~ 'continuous',
      term == 'distToNearest' ~ 'continuous',
      term == 'n_in_quadrant' ~ 'continuous'
    ),
    term_label = case_when(
      term == 'isColSame' ~ 'Is Color the Same',
      term == 'isSizeSame' ~ 'Is Size the Same',
      term == 'isPulseSame' ~ 'Is Pulse the Same',
      term == 'distToMiddleREV' ~ 'Distance to Middle',
      term == 'distToNearest' ~ 'Distance to Nearest',
      term == 'n_in_quadrant' ~ 'N in Quadrant'
    ),
    term_label = factor(term_label, ordered=T, levels=c('Is Color the Same', 'Is Size the Same', 'Is Pulse the Same', 'Distance to Middle', 'Distance to Nearest', 'N in Quadrant')),
  ) %>% 
  arrange(term_class, term_label)

# Plot the model estimate along with the confidence intervals
ggplot(boot_conf_plot, aes(term_label, y=exp(estimate), ymin=exp(conf.low), ymax=exp(conf.high))) +
  geom_point() +
  geom_errorbar(width=0.25) +
  geom_hline(yintercept=1, color='black') +
  scale_y_log10(breaks=c(0.5, 1, 4), limits=c(0.5, 4)) +
  scale_x_discrete(limits=c('Is Size the Same', 'Is Color the Same', 'Is Pulse the Same', 'Distance to Middle', 'Distance to Nearest', 'N in Quadrant')) +
  labs(y='Odds Ratio', x='Term') +
  coord_flip() +
  theme_minimal(base_size=18)
ggsave('../presentation/figures/effect_conf_ints.png')

```



```{r}
# Make a QQ plot of the random effects to see if the normality assumption is reasonable
blups <- ranef(best_model)$subj
colnames(blups) <- c('subj')

ggplot(blups, aes(sample=subj)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title='Subject Random Effects QQ Plot')
ggsave('../presentation/figures/blup_qq_plot.png')

```



## Model Performance
```{r}
# calculate a psuedo R^2 for the selected glmer model
r.squaredGLMM(best_model)

```

```{r}
# Type response doesn't seem to return probabilities, so we need to calculate them manually
model_pred <- augment(best_model, type='response') %>% 
  mutate(
    pred_odds = exp(.fitted),
    pred_prob = pred_odds / (1 + pred_odds),
    pred_class = ifelse(pred_prob > 0.5, 1, 0),
    pred_correct = pred_class == correct
  )

# Calculate the proportion correct (accuracy)
mean(model_pred$pred_correct)

# Calculate the proporion of observations where the subject correctly identified the quadrant
mean(full_df$correct)

```


The model doesn't have particularly high values of R^2. The accuracy isn't particularly high, but is better than random chance since ~53% of the observations were correct.


# Image Analysis

## Proportions

Start with just the proportion correct for each image, plot the top and bottom 8 images
```{r}
# ------------------------------
# Image Analysis
# ------------------------------

# Calculate the proportion correct for each image
img_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    n=n(),
    frac_correct=mean(correct)
  ) %>% 
  arrange(frac_correct)

```


```{r}
# Look at the distribution of the proportion correct by image
ggplot(img_df, aes(frac_correct)) +
  geom_histogram(fill='dodgerblue', color='white') +
  theme_minimal()

# Find the 9 easiest and hardest images
exceptional_images <- bind_rows(
  head(img_df, 9),
  tail(img_df, 9)
) %>% 
  mutate(type=ifelse(frac_correct < 0.5, 'hard', 'easy'))
exceptional_images
length(unique(exceptional_images$currImg))

exceptional_dots <- inner_join(full_dots, exceptional_images, by='currImg') %>% 
  arrange(frac_correct) %>% 
  mutate(
    size = ifelse(cond=='size', 3, 1)
  )

# Plot the hardest images
hard_dots <- exceptional_dots %>% filter(frac_correct < 0.5)
ggplot(hard_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  scale_color_manual(values=c('red', 'navy', 'orange')) +
  labs(title='Hardest Images', color='Group', x='', y='') +
  theme_minimal() +
  scale_x_continuous(breaks=c(0, 100)) +
  scale_y_continuous(breaks=c(0, 100)) +
  theme(panel.grid.minor = element_blank()) +
  guides(size='none')
ggsave('../presentation/figures/hard_images.png')


easy_dots <- exceptional_dots %>% filter(frac_correct > 0.5) %>% arrange(cond)
ggplot(easy_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  scale_color_manual(values=c('red', 'navy', 'orange')) +
  labs(title='Easiest Images', color='Group', x='', y='') +
  theme_minimal() +
  scale_x_continuous(breaks=c(0, 100)) +
  scale_y_continuous(breaks=c(0, 100)) +
  theme(panel.grid.minor = element_blank()) +
  guides(size='none')
  # theme(legend.position='none')
    
ggsave('../presentation/figures/easy_images.png')

```

## Model Analysis


```{r}
# Here we take a look at the effect of the different factors on the odds of the subject correctly identifying the quadrant

# Get prediction data from full_df where we only have one entry per currImg
pred_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    distToMiddleREV = mean(distToMiddleREV),
    distToNearest = mean(distToNearest),
    n_in_quadrant = mean(n_in_quadrant),
    isColSame = mean(isColSame),
    isSizeSame = mean(isSizeSame),
    isPulseSame = mean(isPulseSame)
  ) %>% 
  left_join(img_df, by='currImg')

# Get the coefficients and the average values for the inputs to calculate odds ratios
coefs <- coef(best_model)$subj %>% colMeans()
base_vals <- pred_df %>% select(distToMiddleREV, distToNearest, n_in_quadrant) %>% colMeans()

# Calculate the predicted probability as well as the odds ratios for each of the factors
pred_df <- pred_df %>%
  mutate(
    pred_prob = predict(best_model, newdata=pred_df, type='response', re.form=NA),
    pred_log_odds = predict(best_model, newdata=pred_df, type='link', re.form=NA),
    distToMiddleREVOdds = exp((distToMiddleREV - base_vals[['distToMiddleREV']]) * coefs['distToMiddleREV']),
    distToNearestOdds = exp((distToNearest - base_vals[['distToNearest']]) * coefs['distToNearest']),
    n_in_quadrantOdds = exp((n_in_quadrant - base_vals[['n_in_quadrant']]) * coefs['n_in_quadrant']),
  ) %>% 
  arrange(frac_correct)


# Take a look at the data for the top and bottom few images
pred_df %>% head(9) %>% select(frac_correct, pred_prob, pred_log_odds, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)
pred_df %>% tail(9) %>% select(frac_correct, pred_prob, pred_log_odds, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)

```



# Subject Analysis
```{r}
# ------------------------------
# Subject Analysis
# ------------------------------

# Calculate subject accuracy
subj_acc <- full_df %>% 
  group_by(subj) %>% 
  summarise(
    accuracy = mean(correct)
  )
ggplot(subj_acc, aes(accuracy)) +
  geom_histogram(fill='dodgerblue', color='white') +
  theme_minimal()

```


```{r}
# Filter out all subjects who are either really good or really bad
thresh = 0.3
subj_keep <- subj_acc %>% 
  filter(
    accuracy > thresh 
    & accuracy < 1 - thresh
    )
keep_df <- full_df %>% 
  inner_join(subj_keep, by='subj')

# Fit a model without the "outlier" subjects
no_outlier_model <- glmer(
  correct~distToMiddleREV + distToNearest + n_in_quadrant + isColSame + isSizeSame + isPulseSame + (1|subj), 
  data = keep_df, 
  family = binomial,
  nAGQ=1,
  control=glmerControl(optimizer='bobyqa')
  )
summary(no_outlier_model)

```


## Subject Performance Bias

Take a look to see if we had more bad or really good subjects working on some images which might lead us to believing that an image was easier or harder than it actually was

```{r}
# Merge the subject accuracy with the image data to make box plots to see if the easiest or hardest images had more good or bad subjects working on them.
subj_img_df <- inner_join(
  full_df,
  exceptional_images %>% select(currImg, frac_correct, type),
  by='currImg'
) %>% 
  left_join(subj_acc, by='subj')

ggplot(subj_img_df, aes(factor(currImg), accuracy, fill=type)) +
  facet_grid(rows=vars(type), scales='free') + 
  geom_boxplot() +
  theme_minimal()

```

# Appendix B - Table of Image Proportions
```{r}
# Provide a table showing how many subjects saw each image and the proportion that correctly identified the quadrant
img_df %>% 
  kable(col.names=c('Image ID', 'N Subjects', 'Proportion Correct'))

```


# Appendix

```{r show-code, ref.label = all_labels(), echo = TRUE, eval = FALSE}
```