---
title: "Visual Analysis"
author: "Rodney Murray"
format: html
---

```{r}
library(tidyverse)
library(lme4)
library(broom)
library(MuMIn)
library(broom.mixed)
```

# Data Loading and Processing


## Load Data
```{r}
df <- read.csv('../src/data/WittData_AHRev052324.csv')
dots <- read.csv('../src/data/Witt_Dots.csv')
```

## Processing
Calculate which quadrant each dot is in
```{r}
dots <- dots %>% mutate(
  quadrant = case_when(
  x < 50 & y >= 50 ~ 'A',
  x >= 50 & y >= 50 ~ 'B',
  x < 50 & y < 50 ~ 'C',
  x >= 50 & y < 50 ~ 'D',
  )
)
```

Calculate the mean values for x and y and which quadrant the centroid is in
```{r}
mean_dots <- dots %>% 
  group_by(currImg) %>% 
  summarise(meanx = mean(x), meany = mean(y)) %>% 
  mutate(
      mean_quadrant = case_when(
        meanx < 50 & meany >= 50 ~ 'A',
        meanx >= 50 & meany >= 50 ~ 'B',
        meanx < 50 & meany < 50 ~ 'C',
        meanx >= 50 & meany < 50 ~ 'D',
    )
  )
```

Join the centroid and associated quadrant back to the dots
```{r}
full_dots <- left_join(dots, mean_dots, by = 'currImg') %>% 
  mutate(
    in_mean_quadrant = quadrant == mean_quadrant
    )
head(full_dots)

# Calculate the number of dots in the same quadrant as the centroid
mean_full_dots <- full_dots %>% 
  group_by(currImg) %>% 
  summarise(n_in_quadrant = sum(in_mean_quadrant))
head(mean_full_dots)
```

Calculate if the special groups are in the same quadrant as the mean of all
```{r}
group_dots <- dots %>% 
  group_by(currImg, cond) %>% 
  summarize(meanx=mean(x), meany=mean(y)) %>% 
  mutate(
    group_quadrant = case_when(
      meanx < 50 & meany >= 50 ~ 'A',
      meanx >= 50 & meany >= 50 ~ 'B',
      meanx < 50 & meany < 50 ~ 'C',
      meanx >= 50 & meany < 50 ~ 'D',
    )
  )%>% 
  pivot_wider(
    id_cols=currImg,
    names_from=cond,
    values_from=group_quadrant
  )
group_dots %>% head()
```



Join the dot data back with the response data
```{r}
full_df <- left_join(df, mean_full_dots, by = 'currImg') %>% 
  left_join(group_dots, by='currImg') %>%
  mutate(
    correct = as.numeric(resp == corrQuadREV),
    distToNearest = pmin(abs(meanx -50), abs(meany - 50)),
    isColSameREV = as.numeric(color == corrQuadREV),
    isSizeSameREV = as.numeric(size == corrQuadREV),
    isPulseSameREV = as.numeric(pulse == corrQuadREV)
    )
full_df %>% head() %>% select(isColSame, isColSameREV, correct)
```


Interestingly enough it seems that the special groups were correctly calculated
```{r}
full_df %>% 
  mutate(
    color = isColSame == isColSameREV,
    size = isSizeSame == isSizeSameREV,
    pulse = isPulseSame == isPulseSameREV
    ) %>% 
  select(color, size, pulse, isColSameREV, isSizeSameREV, isPulseSameREV) %>% 
  summary()
```


Plot our data to check that we correctly identified the quadrants for the centroids
```{r}
ggplot(df, aes(meanx, meany, color=corrQuadREV)) +
  geom_point()
```

# EDA

```{r}
cols <- c('distToNearest', 'distToMiddleREV', 'n_in_quadrant')


df_summary <- full_df %>% 
  mutate(dist_bin = cut(distToMiddleREV, breaks = 20)) %>% 
  group_by(dist_bin) %>%
  summarise(
    prop_correct = mean(correct),
    mea_dist = mean(distToMiddleREV)
    )

# Iterate over each column in cols and calculate the proportion correct for binned values of each column
summaries <- data.frame()
for (i in 1:length(cols)) {
  col <- cols[i]
  df_summary <- full_df %>% 
    mutate(col_bin = cut(!!sym(col), breaks = 15)) %>% 
    group_by(col_bin) %>%
    summarise(
      prop_correct = mean(correct),
      mean_value = mean(!!sym(col)),
      col_name = col
    )
  summaries <- bind_rows(summaries, df_summary)
}
```


```{r}
ggplot(summaries, aes(mean_value, prop_correct)) +
  facet_wrap(~col_name, scales='free_x', ncol=2) +
  geom_point() +
  theme_minimal()
```

# Modeling

I left out `howManyCorr` becuase of collinearity, and the currImg due to convergence issues.

Not too worried about currImg being out since we have other variables which should help describe the data reasonbly well.

```{r}
formula <- correct ~ 
  distToMiddleREV + 
  n_in_quadrant +
  meanSD +
  distToNearest +
  isColSame +
  isSizeSame +
  isPulseSame +
  # howManyCorr +
  # (1|currImg) +
  (1|subj)

model <- glmer(
  formula=formula,
  data = full_df,
  family = binomial,
  )
summary(model)
```


## Model Selection

### Stepwise - Forward

```{r}
base_model <- glmer(
  correct ~ isColSame + isSizeSame + isPulseSame + (1|subj),
  data = full_df,
  family = binomial
  )
base_model
```


```{r}
# Apparently this doesn't work but "dredge" does? 
# scope = list(lower=base_model)
# forward_model <- step(model, scope=scope, direction='forward')
```


### Dredge

```{r}
options(na.action='na.fail')
dredge_res <- dredge(model, fixed=c('isColSame', 'isPulseSame', 'isSizeSame'))
dredge_res
```


```{r}
best_model <- get.models(dredge_res, 1)[[1]]
summary(best_model)
```

```{r}
# Get a sample of the full dataset full_df and fit a model, use this to test out the confidence interval algorithms.
sample_index <- sample(1:nrow(full_df), 1000, replace=F)
sample_df <- full_df[sample_index,]

sample_model <- glmer(
  correct ~ distToMiddleREV + distToNearest + n_in_quadrant + isColSame + isSizeSame + isPulseSame + (1|subj), 
  data = sample_df, 
  family = binomial
  )
summary(sample_model)
```

```{r}
wald_confints <- tidy(best_model, conf.int=T, conf.method='Wald')
conf_plot <- wald_confints %>% filter(!(term %in% c('(Intercept)', 'sd__(Intercept)')))

ggplot(conf_plot, aes(term, y=exp(estimate), ymin=exp(conf.low), ymax=exp(conf.high))) +
  geom_point() +
  geom_errorbar(width=0.25) +
  geom_hline(yintercept=1, color='black') +
  scale_y_log10() +
  labs(y='Odds Ratio', y='Term') +
  coord_flip() +
  theme_minimal()
```

```{r}
full_df %>% select(distToNearest, distToMiddleREV, n_in_quadrant) %>% colMeans()

full_df %>% select(distToNearest, distToMiddleREV, n_in_quadrant) %>% summary()
```


```{r}
# This takes a while to run
conf_ints <- confint(
  best_model, 
  oldNames=F, 
  method='boot', 
  parallel='multicore',
  ncpus=8,
  .progress='txt'
  )
conf_ints
```

```{r}
conf_ints
# save(conf_ints, file='conf_ints.RData')
```

```{r}
conf_df <- as.data.frame(conf_ints)
colnames(conf_df) <- c('conf.low', 'conf.high')
conf_df <- conf_df %>% rownames_to_column('term')
boot_confint <- tidy(best_model) %>% left_join(conf_df, by='term')
```

```{r}
boot_conf_plot <- boot_confint %>% filter(!(term %in% c('(Intercept)', 'sd__(Intercept)')))
ggplot(boot_conf_plot, aes(term, y=exp(estimate), ymin=exp(conf.low), ymax=exp(conf.high))) +
  geom_point() +
  geom_errorbar(width=0.25) +
  geom_hline(yintercept=1, color='black') +
  scale_y_log10() +
  labs(y='Odds Ratio', x='Term') +
  coord_flip() +
  theme_minimal()
ggsave('../presentation/figures/effect_conf_ints.png')
```

## Model Performance
```{r}
# calculate a psuedo R^2 for the glmer model
r.squaredGLMM(model)
```

```{r}
# Type response doesn't seem to return probabilities
model_pred <- augment(best_model, type='response') %>% 
  mutate(
    pred_odds = exp(.fitted),
    pred_prob = pred_odds / (1 + pred_odds),
    pred_class = ifelse(pred_prob > 0.5, 1, 0),
    pred_correct = pred_class == correct
  )

# Calculate the proportion correct (accuracy)
mean(model_pred$pred_correct)
```

```{r}
# Calculate the proportion correct for the full dataset
full_df %>% 
  summarise(n=n(), frac_correct=mean(correct))
```


The model doesn't have particularly high values of R^2. The accuracy isn't particularly high, but is better than random chance since ~53% of the observations were correct.

## Model Checks

```{r}
preds <- data.frame()
cols <- c('distToMiddleREV', 'distToNearest', 'n_in_quadrant')
for (col in cols) {
  df <- ggpredict(best_model, terms=c(paste(col, '[all]', sep=''))) %>% 
    as.data.frame() %>% 
    mutate(col_name=col)
  preds <- bind_rows(preds, df)
}
preds %>% head()
```




```{r}
ggplot(summaries, aes(mean_value, prop_correct)) +
  facet_wrap(~col_name, scales='free_x', ncol=2) +
  geom_point() +
  geom_line(data=preds, aes(x=x, y=predicted), color='firebrick') +
  theme_minimal()
```

# Image Analysis

## Proportions

Start with just the proportion correct for each image, plot the top and bottom 8 images
```{r}
img_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    n=n(),
    frac_correct=mean(correct)
  ) %>% 
  arrange(frac_correct)

ggplot(img_df, aes(frac_correct)) +
  geom_histogram(fill='dodgerblue', color='white') +
  theme_minimal()

exceptional_images <- bind_rows(
  head(img_df, 9),
  tail(img_df, 9)
)
exceptional_images
length(unique(exceptional_images$currImg))

exceptional_dots <- inner_join(full_dots, exceptional_images, by='currImg') %>% 
  arrange(frac_correct) %>% 
  mutate(
    size = ifelse(cond=='size', 3, 1)
  )

hard_dots <- exceptional_dots %>% filter(frac_correct < 0.5)
ggplot(hard_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  labs(title='Hardest Images', color='Group', x='', y='') +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  guides(size='none')
  # theme(legend.position='none')
    
ggsave('../presentation/figures/hard_images.png')

hard_dots <- exceptional_dots %>% filter(frac_correct > 0.5)
ggplot(hard_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  labs(title='Easiest Images', color='Group', x='', y='') +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  guides(size='none')
  # theme(legend.position='none')
    
ggsave('../presentation/figures/easy_images.png')
```

## Model Analysis


```{r}
# Get prediction data from full_df where we only have one intery per currImg
pred_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    distToMiddleREV = mean(distToMiddleREV),
    distToNearest = mean(distToNearest),
    n_in_quadrant = mean(n_in_quadrant),
    isColSame = mean(isColSame),
    isSizeSame = mean(isSizeSame),
    isPulseSame = mean(isPulseSame)
  ) %>% 
  left_join(img_df, by='currImg')

# Get the coefficients and the average values for the inputs to calculate odds ratios
coefs <- coef(best_model)$subj %>% colMeans()


# Calculate the predicted probability as well as the odds ratios for each of the factors
pred_df <- pred_df %>%
  mutate(
    pred_prob = predict(best_model, newdata=pred_df, type='response', re.form=NA),
    pred_log_odds = predict(best_model, newdata=pred_df, type='link', re.form=NA),
    distToMiddleREVOdds = exp((distToMiddleREV - base_vals[['distToMiddleREV']]) * coefs['distToMiddleREV']),
    distToNearestOdds = exp((distToNearest - base_vals[['distToNearest']]) * coefs['distToNearest']),
    n_in_quadrantOdds = exp((n_in_quadrant - base_vals[['n_in_quadrant']]) * coefs['n_in_quadrant']),
  ) %>% 
  arrange(frac_correct)


# Take a look at the data for the top and bottom few images
pred_df %>% head() %>% select(frac_correct, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)
pred_df %>% tail() %>% select(frac_correct, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)
```

```{r}
ggplot(pred_df, aes(frac_correct, pred_prob)) +
  geom_point() +
  theme_minimal()
```


$$
\log(\frac {p} {1 - p}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 \\

\frac {p} {1 - p} = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3} \\

\frac {p} {1 - p} = e^{\beta_0} \times e^{\beta_1 x_1} \times e^{\beta_2 x_2} \times e^{\beta_3 x_3}
$$

