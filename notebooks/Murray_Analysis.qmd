---
title: "Visual Analysis"
author: "Rodney Murray"
format: html
---

```{r}
library(tidyverse)
library(lme4)
library(broom)
library(MuMIn)
library(broom.mixed)
library(GGally)
```

# Data Loading and Processing


## Load Data
```{r}
df <- read.csv('../src/data/WittData_AHRev052324.csv')
dots <- read.csv('../src/data/Witt_Dots.csv')
```

## Processing
Calculate which quadrant each dot is in
```{r}
dots <- dots %>% mutate(
  quadrant = case_when(
  x < 50 & y >= 50 ~ 'A',
  x >= 50 & y >= 50 ~ 'B',
  x < 50 & y < 50 ~ 'C',
  x >= 50 & y < 50 ~ 'D',
  )
)
```

Calculate the mean values for x and y and which quadrant the centroid is in
```{r}
mean_dots <- dots %>% 
  group_by(currImg) %>% 
  summarise(meanx = mean(x), meany = mean(y)) %>% 
  mutate(
      mean_quadrant = case_when(
        meanx < 50 & meany >= 50 ~ 'A',
        meanx >= 50 & meany >= 50 ~ 'B',
        meanx < 50 & meany < 50 ~ 'C',
        meanx >= 50 & meany < 50 ~ 'D',
    )
  )
```

Join the centroid and associated quadrant back to the dots
```{r}
full_dots <- left_join(dots, mean_dots, by = 'currImg') %>% 
  mutate(
    in_mean_quadrant = quadrant == mean_quadrant
    )
head(full_dots)

# Calculate the number of dots in the same quadrant as the centroid
mean_full_dots <- full_dots %>% 
  group_by(currImg) %>% 
  summarise(n_in_quadrant = sum(in_mean_quadrant), n=n())
head(mean_full_dots)
```


Calculate if the special groups are in the same quadrant as the mean of all
```{r}
group_dots <- dots %>% 
  group_by(currImg, cond) %>% 
  summarize(meanx=mean(x), meany=mean(y)) %>% 
  mutate(
    group_quadrant = case_when(
      meanx < 50 & meany >= 50 ~ 'A',
      meanx >= 50 & meany >= 50 ~ 'B',
      meanx < 50 & meany < 50 ~ 'C',
      meanx >= 50 & meany < 50 ~ 'D',
    )
  )%>% 
  pivot_wider(
    id_cols=currImg,
    names_from=cond,
    values_from=group_quadrant
  )
group_dots %>% head()
```



Join the dot data back with the response data
```{r}
full_df <- left_join(df, mean_full_dots, by = 'currImg') %>% 
  left_join(group_dots, by='currImg') %>%
  mutate(
    correct = as.numeric(resp == corrQuadREV),
    distToNearest = pmin(abs(meanx -50), abs(meany - 50)),
    isColSameREV = as.numeric(color == corrQuadREV),
    isSizeSameREV = as.numeric(size == corrQuadREV),
    isPulseSameREV = as.numeric(pulse == corrQuadREV)
    )
full_df %>% head() %>% select(isColSame, isColSameREV, correct)
```


Interestingly enough it seems that the special groups were correctly calculated
```{r}
full_df %>% 
  mutate(
    color = isColSame == isColSameREV,
    size = isSizeSame == isSizeSameREV,
    pulse = isPulseSame == isPulseSameREV
    ) %>% 
  select(color, size, pulse, isColSameREV, isSizeSameREV, isPulseSameREV) %>% 
  summary()
```




Plot our data to check that we correctly identified the quadrants for the centroids
```{r}
ggplot(df, aes(meanx, meany, color=corrQuadREV)) +
  geom_point()
```

## New Variable Explanation

```{r}
single_img <- full_dots %>% filter(currImg == 4)

ggplot(single_img, aes(x, y, color=in_mean_quadrant)) +
  geom_point() +
  labs(title='Example of N-in-quadrant', color='') +
  geom_point(aes(x=meanx, y=meany, color='Centroid'), size=3) +
  scale_color_manual(values=c('dodgerblue', 'grey', 'red')) +
  geom_vline(xintercept=50, color='black') +
  geom_hline(yintercept=50, color='black') +
  lims(x=c(0, 100), y=c(0, 100)) +
  theme_minimal(base_size=18)
ggsave('../presentation/figures/n_in_quadrant.png')
```

```{r}
full_df %>% filter(currImg == 4) %>% select(n_in_quadrant, correct) %>% head()
```



# EDA

```{r}
cols <- c('distToNearest', 'distToMiddleREV', 'n_in_quadrant')


df_summary <- full_df %>% 
  mutate(dist_bin = cut(distToMiddleREV, breaks = 20)) %>% 
  group_by(dist_bin) %>%
  summarise(
    prop_correct = mean(correct),
    mea_dist = mean(distToMiddleREV)
    )

# Iterate over each column in cols and calculate the proportion correct for binned values of each column
summaries <- data.frame()
for (i in 1:length(cols)) {
  col <- cols[i]
  df_summary <- full_df %>% 
    mutate(col_bin = cut(!!sym(col), breaks = 15)) %>% 
    group_by(col_bin) %>%
    summarise(
      prop_correct = mean(correct),
      mean_value = mean(!!sym(col)),
      col_name = col
    )
  summaries <- bind_rows(summaries, df_summary)
}
```


```{r}
ggplot(summaries, aes(mean_value, prop_correct)) +
  facet_wrap(~col_name, scales='free_x', ncol=2) +
  geom_point() +
  theme_minimal()
```

Take a look at correlation between fixed effects
```{r}
ggcorr(full_df %>% select(distToMiddleREV, distToNearest, n_in_quadrant, isColSame, isSizeSame, isPulseSame))
ggsave('../presentation/figures/fixed_effects_correlation.png')
```


# Modeling

I left out `howManyCorr` becuase of collinearity, and the currImg due to convergence issues.

Not too worried about currImg being out since we have other variables which should help describe the data reasonbly well.

```{r}
formula <- correct ~ 
  distToMiddleREV + 
  n_in_quadrant +
  meanSD +
  distToNearest +
  isColSame +
  isSizeSame +
  isPulseSame +
  # howManyCorr +
  # (1|currImg) +
  (1|subj)

model <- glmer(
  formula=formula,
  data = full_df,
  family = binomial,
  # control=glmerControl(optimizer='bobyqa')
  )
summary(model)
```

```{r}
# calculate a psuedo R^2 for the glmer model
r.squaredGLMM(model)
```

## Model Selection

### Dredge


```{r}
options(na.action='na.fail')
dredge_res <- dredge(model, fixed=c('isColSame', 'isPulseSame', 'isSizeSame'))
dredge_res
```


```{r}
best_model <- get.models(dredge_res, 1)[[1]]
summary(best_model)
```


```{r}
full_df %>% select(distToNearest, distToMiddleREV, n_in_quadrant) %>% summary()

full_df %>% summarize(
  distToNearestMean = mean(distToNearest), 
  distToNearestSD = sd(distToNearest),
  distToMiddleREVMean = mean(distToMiddleREV),
  distToMiddleREVSD = sd(distToMiddleREV),
  n_in_quadrantMean = mean(n_in_quadrant),
  n_in_quadrantSD = sd(n_in_quadrant)
  )
```


```{r}
# This takes a while to run
conf_ints <- confint(
  best_model, 
  oldNames=F, 
  method='boot', 
  parallel='multicore',
  ncpus=8,
  .progress='txt'
  )
conf_ints
```

```{r}
conf_ints
# save(conf_ints, file='conf_ints.RData')
```

```{r}
conf_df <- as.data.frame(conf_ints)
colnames(conf_df) <- c('conf.low', 'conf.high')
conf_df <- conf_df %>% rownames_to_column('term')
boot_confint <- tidy(best_model) %>% left_join(conf_df, by='term')
```

```{r}
boot_conf_plot <- boot_confint %>% 
  filter(!(term %in% c('(Intercept)', 'sd__(Intercept)'))) %>% 
  mutate(
    term_class = case_when(
      term == 'isColSame' ~ 'constant',
      term == 'isSizeSame' ~ 'constant',
      term == 'isPulseSame' ~ 'constant',
      term == 'distToMiddleREV' ~ 'continuous',
      term == 'distToNearest' ~ 'continuous',
      term == 'n_in_quadrant' ~ 'continuous'
    ),
    term_label = case_when(
      term == 'isColSame' ~ 'Is Color the Same',
      term == 'isSizeSame' ~ 'Is Size the Same',
      term == 'isPulseSame' ~ 'Is Pulse the Same',
      term == 'distToMiddleREV' ~ 'Distance to Middle',
      term == 'distToNearest' ~ 'Distance to Nearest',
      term == 'n_in_quadrant' ~ 'N in Quadrant'
    ),
    term_label = factor(term_label, ordered=T, levels=c('Is Color the Same', 'Is Size the Same', 'Is Pulse the Same', 'Distance to Middle', 'Distance to Nearest', 'N in Quadrant')),
  ) %>% 
  arrange(term_class, term_label)

ggplot(boot_conf_plot, aes(term_label, y=exp(estimate), ymin=exp(conf.low), ymax=exp(conf.high))) +
  geom_point() +
  geom_errorbar(width=0.25) +
  geom_hline(yintercept=1, color='black') +
  scale_y_log10(breaks=c(0.5, 1, 4), limits=c(0.5, 4)) +
  scale_x_discrete(limits=c('Is Size the Same', 'Is Color the Same', 'Is Pulse the Same', 'Distance to Middle', 'Distance to Nearest', 'N in Quadrant')) +
  labs(y='Odds Ratio', x='Term') +
  coord_flip() +
  theme_minimal(base_size=18)
ggsave('../presentation/figures/effect_conf_ints.png')
```



```{r}
blups <- ranef(best_model)$subj
colnames(blups) <- c('subj')

ggplot(blups, aes(sample=subj)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title='Subject Random Effects QQ Plot')
```



## Model Performance
```{r}
# calculate a psuedo R^2 for the glmer model
r.squaredGLMM(best_model)
```

```{r}
# Type response doesn't seem to return probabilities
model_pred <- augment(best_model, type='response') %>% 
  mutate(
    pred_odds = exp(.fitted),
    pred_prob = pred_odds / (1 + pred_odds),
    pred_class = ifelse(pred_prob > 0.5, 1, 0),
    pred_correct = pred_class == correct
  )

# Calculate the proportion correct (accuracy)
mean(model_pred$pred_correct)
```

```{r}
mean(full_df$correct)
```


The model doesn't have particularly high values of R^2. The accuracy isn't particularly high, but is better than random chance since ~53% of the observations were correct.

## Model Checks

```{r}
# preds <- data.frame()
# cols <- c('distToMiddleREV', 'distToNearest', 'n_in_quadrant')
# for (col in cols) {
#   df <- ggpredict(best_model, terms=c(paste(col, '[all]', sep=''))) %>% 
#     as.data.frame() %>% 
#     mutate(col_name=col)
#   preds <- bind_rows(preds, df)
# }
# preds %>% head()
```




```{r}
# ggplot(summaries, aes(mean_value, prop_correct)) +
#   facet_wrap(~col_name, scales='free_x', ncol=2) +
#   geom_point() +
#   geom_line(data=preds, aes(x=x, y=predicted), color='firebrick') +
#   theme_minimal()
```

# Image Analysis

## Proportions

Start with just the proportion correct for each image, plot the top and bottom 8 images
```{r}
img_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    n=n(),
    frac_correct=mean(correct)
  ) %>% 
  arrange(frac_correct)

ggplot(img_df, aes(frac_correct)) +
  geom_histogram(fill='dodgerblue', color='white') +
  theme_minimal()

exceptional_images <- bind_rows(
  head(img_df, 9),
  tail(img_df, 9)
) %>% 
  mutate(type=ifelse(frac_correct < 0.5, 'hard', 'easy'))
exceptional_images
length(unique(exceptional_images$currImg))

exceptional_dots <- inner_join(full_dots, exceptional_images, by='currImg') %>% 
  arrange(frac_correct) %>% 
  mutate(
    size = ifelse(cond=='size', 3, 1)
  )

hard_dots <- exceptional_dots %>% filter(frac_correct < 0.5)
ggplot(hard_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  scale_color_manual(values=c('red', 'navy', 'orange')) +
  labs(title='Hardest Images', color='Group', x='', y='') +
  theme_minimal() +
  scale_x_continuous(breaks=c(0, 100)) +
  scale_y_continuous(breaks=c(0, 100)) +
  theme(panel.grid.minor = element_blank()) +
  guides(size='none')
  # theme(legend.position='none')
    
ggsave('../presentation/figures/hard_images.png')

hard_dots <- exceptional_dots %>% filter(frac_correct > 0.5) %>% arrange(cond)
ggplot(hard_dots, aes(x, y, color=cond, size=size)) +
  facet_wrap(~currImg, ncol=3) +
  geom_point() +
  scale_size(range=c(1,2)) +
  scale_color_manual(values=c('red', 'navy', 'orange')) +
  labs(title='Easiest Images', color='Group', x='', y='') +
  theme_minimal() +
  scale_x_continuous(breaks=c(0, 100)) +
  scale_y_continuous(breaks=c(0, 100)) +
  theme(panel.grid.minor = element_blank()) +
  guides(size='none')
  # theme(legend.position='none')
    
ggsave('../presentation/figures/easy_images.png')
```

## Model Analysis


```{r}
# Get prediction data from full_df where we only have one intery per currImg
pred_df <- full_df %>% 
  group_by(currImg) %>% 
  summarise(
    distToMiddleREV = mean(distToMiddleREV),
    distToNearest = mean(distToNearest),
    n_in_quadrant = mean(n_in_quadrant),
    isColSame = mean(isColSame),
    isSizeSame = mean(isSizeSame),
    isPulseSame = mean(isPulseSame)
  ) %>% 
  left_join(img_df, by='currImg')

# Get the coefficients and the average values for the inputs to calculate odds ratios
coefs <- coef(best_model)$subj %>% colMeans()
base_vals <- pred_df %>% select(distToMiddleREV, distToNearest, n_in_quadrant) %>% colMeans()

# Calculate the predicted probability as well as the odds ratios for each of the factors
pred_df <- pred_df %>%
  mutate(
    pred_prob = predict(best_model, newdata=pred_df, type='response', re.form=NA),
    pred_log_odds = predict(best_model, newdata=pred_df, type='link', re.form=NA),
    distToMiddleREVOdds = exp((distToMiddleREV - base_vals[['distToMiddleREV']]) * coefs['distToMiddleREV']),
    distToNearestOdds = exp((distToNearest - base_vals[['distToNearest']]) * coefs['distToNearest']),
    n_in_quadrantOdds = exp((n_in_quadrant - base_vals[['n_in_quadrant']]) * coefs['n_in_quadrant']),
  ) %>% 
  arrange(frac_correct)


# Take a look at the data for the top and bottom few images
pred_df %>% head(9) %>% select(frac_correct, pred_prob, pred_log_odds, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)
pred_df %>% tail(9) %>% select(frac_correct, pred_prob, pred_log_odds, distToMiddleREVOdds, distToNearestOdds, n_in_quadrantOdds)
```


## Take a look at fitting a model where the extereme performers are no longer included
```{r}
# Calculate subject accuracy
subj_acc <- full_df %>% 
  group_by(subj) %>% 
  summarise(
    accuracy = mean(correct)
  )
ggplot(subj_acc, aes(accuracy)) +
  geom_histogram(fill='dodgerblue', color='white') +
  theme_minimal()
```


```{r}
thresh = 0.3
subj_keep <- subj_acc %>% 
  filter(
    accuracy > thresh 
    & accuracy < 1 - thresh
    )
keep_df <- full_df %>% 
  inner_join(subj_keep, by='subj')

# keep_df %>% head()

no_outlier_model <- glmer(
  correct~distToMiddleREV + distToNearest + n_in_quadrant + isColSame + isSizeSame + isPulseSame + (1|subj), 
  data = keep_df, 
  family = binomial,
  nAGQ=1,
  control=glmerControl(optimizer='bobyqa')
  )
summary(no_outlier_model)
```


## Subject Performance Bias

Take a look to see if we had more bad or really good subjects working on some images which might lead us to believing that an image was easuer or harder than it actually was

```{r}
subj_img_df <- inner_join(
  full_df,
  exceptional_images %>% select(currImg, frac_correct, type),
  by='currImg'
) %>% 
  left_join(subj_acc, by='subj')

ggplot(subj_img_df, aes(factor(currImg), accuracy, fill=type)) +
  facet_grid(rows=vars(type), scales='free') + 
  geom_boxplot() +
  theme_minimal()
```



